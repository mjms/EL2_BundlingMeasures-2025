{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline for *\"Bundling Measures for Food Systems Transformation: a global, multimodel assessment\"*\n",
    "This outlines the data processing for ***\"Bundling measures for food systems transformation: a global multimodel assessment\"***, by Sundiang et al, 2025, *The Lancet Planetary Health*\n",
    "\n",
    "This uses the custom package AgMIP Processing PipeLinE or `applepy`. Note that `applepy` is still in development. The version used here may not be the most current release. Therefore it is important to use the included `applepy` folder in this repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import applepy as apy\n",
    "from applepy.utils.calculations.basic import *\n",
    "from applepy.utils.calculations.bias_correction import *\n",
    "from applepy.utils.calculations.emissions import *\n",
    "from applepy.utils.calculations.land import *\n",
    "\n",
    "from applepy.utils.preprocessing.merge import *\n",
    "from applepy.utils.preprocessing.checks import *\n",
    "from applepy.pipeline.pipeline import *\n",
    "from applepy.visualization.coverage_map import *\n",
    "\n",
    "from applepy.utils.helper import * \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Project Setup\n",
    "0.1 Update the AgMIP template with relevant indicators to the specific project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: myGeoHub\n",
    "The first step of the processing is done through the AgMIP GlobalEcon Data Submission tool (Zhao et al, 2023), hosted through MyGeoHub.\n",
    "\n",
    "1. Each AgMIP project should be setup with its own `RulesTables.xlsx` file. The `RulesTables` file is a lookup table for the submission tool to validate the submission's indicators, units, scenario names, etc. to make sure that it follows the AgMIP reporting convention.\n",
    "This `RulesTables` file has the following sheets\n",
    "* 'Version'\n",
    "* 'ModelTable'\n",
    "* 'ScenarioTable'\n",
    "* 'RegionFixTable'\n",
    "* 'VariableTable'\n",
    "* 'VariableUnitValueTable'\n",
    "* 'ItemTable'\n",
    "* 'UnitTable'\n",
    "* 'YearTable'\n",
    "* 'ValueFixTable'\n",
    "* 'RegionTable\n",
    "\n",
    "2. A validation step in the submission tool allows the user to override or replace values that do not match the `RulesTables`. Here one can choose among the allowed values, and the submission tool will replace the invalid values. If there is no corresponding valid value for the item that caused the exception, one can select to override the exception. Submissions with exceptions are moved to the pending folder and needs manual review (which is discussed in the next step)\n",
    "\n",
    "3. The pending submissions will have two files: the original file and another overrides file (`*_OVERRIDES.csv`)\n",
    "\n",
    "**References:**\n",
    " Lan Zhao; Jaewoo Shin; Rob Campbell; Raziq Ramli (2023), \"AgMIP GlobalEcon Data Submission,\" https://mygeohub.org/resources/agmipup. (DOI: 10.21981/ZTHG-RK49)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setting up applepy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. Install the python environment file:**\n",
    "\n",
    "`conda env create -n el-modelling_v3 -f el-modelling_v3.yml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import applepy as apy\n",
    "from applepy.utils.calculations.basic import *\n",
    "from applepy.utils.calculations.bias_correction import *\n",
    "from applepy.utils.calculations.emissions import *\n",
    "from applepy.utils.calculations.land import *\n",
    "\n",
    "from applepy.utils.preprocessing.merge import *\n",
    "from applepy.utils.preprocessing.checks import *\n",
    "from applepy.pipeline.pipeline import *\n",
    "from applepy.visualization.coverage_map import *\n",
    "\n",
    "from applepy.utils.helper import * \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Generating the `*_OVERRIDES_fix` file.** \n",
    "\n",
    "The original file that comes from myGeoHub has the following structure (without the headers):\n",
    "<center>\n",
    "\n",
    "| override | column | closest value |\n",
    "|----------|--------|--------|\n",
    "| YILD_endoTC | Variable | YILD |\n",
    "| Nr/Nr | Unit | TgN/year |\n",
    "| unitless | Unit | index |\n",
    "| t dm/ha | Unit | t/ha|\n",
    " </center>\n",
    "\n",
    "we replace the last column with a `status` column and generate an `*_OVERRIDES_fix.csv` file\n",
    "<center>\n",
    "\n",
    "| override | column | status |\n",
    "|----------|--------|--------|\n",
    "| YILD_endoTC | Variable | False |\n",
    "| Nr/Nr | Unit | True |\n",
    "| unitless | Unit | True |\n",
    "| t dm/ha | Unit | dm t/ha|\n",
    "</center>\n",
    "\n",
    "The `status` column takes in a boolean value or a replacement value. A False value will remove the entries from the dataset (row 1 above), and True will keep the entries (rows 2 and 3). If the value is not boolean and is a string datatype, the value will be replaced by the string (row 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```python\n",
    "data_dir = '../data/'\n",
    "fps = sorted([pjoin(data_dir,x) for x in os.listdir(data_dir) if (x.endswith('OVERRIDES.csv'))])\n",
    "col_names = ['override','column','status']\n",
    "\n",
    "for fp in fps:\n",
    "    override_df = pd.read_csv(fp, names=col_names)\n",
    "    override_df['status'] = False\n",
    "    save_fp = fp.split('/')[-1].split('.csv')[0]+'_fix.csv'\n",
    "    override_df.to_csv(pjoin(data_dir,save_fp),index=False,header=False)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Running the EL2 Pipeline\n",
    "### On a single submission ([source](../applepy/pipeline/pipeline.py#L16))\n",
    "**0.** ([source](../applepy/utils/helper.py#L94)) The model csv file is read into a Pandas DataFrame. This step assumes that the model submission has already been passed through the AgMIP submission tool above (or has the same format, i.e. no headers, comma separated, and has the column order `['model','scenario','region','variable','item','unit','year','value']`). All outputs will be saved in the same folder as the file and will be named with the model name as the prefix.\n",
    "\n",
    "**1. Checking duplicates.** ([source](../applepy/utils/preprocessing/checks.py#L4))\n",
    "Duplicated entries that match all values in the column are dropped (only one copy of the entry is kept), while entries that have a different values (`scenario, region, variable, item, unit and year` are the same) are separated from the clean DataFrame into a `<model>_duplicates.csv` file. This file is saved in a new folder `<location of model csv file>/duplicates`.\n",
    "\n",
    "**2. Setting aside variables to keep** \n",
    "This step looks at the `Keep` column in the `RulesTable.xlsx` sheet `'VariableUnitValueTable'`. All the variables that are marked `True` in this column are set aside and added back *after* steps 3 and 4\n",
    "\n",
    "**3. Checking overrides** ([source](../applepy/utils/preprocessing/checks.py#L37))\n",
    "This steps handles the overrides according to the `*_OVERRIDES_fix.csv` file. If this file does not exist, the entire DataFrame is kept as is. Values that are set to `False`, and the list of overridden entries are kept in `*_overrides-list.csv`. This file is saved in a new folder `<location of model csv file>/overrides`.Values that are set to `True` are kept in the DataFrame as is. And Values that are neither are replaced by the value in the `status` column. \n",
    "\n",
    "**4. Checking against template** ([source](../applepy/utils/preprocessing/checks.py#L78))\n",
    "The DataFrame is checked against the `RulesTable.xlsx` sheet `'VariableUnitValueTable'`. This is the final check whether each variable has the correct expected unit. Exeptions are removed and moved to ``<location of model csv file>/template-checked/*_template-exceptions-list.csv'`\n",
    "\n",
    "**5. Calculating percentage changes** ([source](../applepy/utils/calculations/bias_correction.p#L137))\n",
    "\n",
    "* Get base year (for this project we used 2020). If the model does not report that base year, interpolate using values before and after the base year (the submission should have years reported ***before and after**)\n",
    "* The DataFrame is grouped by `'model','variable','item','region','unit'` in order to calculate the percentage change from the base year, and the same years across scenarios. \n",
    "* Additional columns are created:\n",
    "    * `'BAU_ref_year'`: base year\n",
    "    * `'percent_change_BAU_ref_year'`: percentage change from base year\n",
    "    * `'diff_BAU_ref_year'`: absolute difference between scenario and year from BAU base year\n",
    "    * `'percent_change_BAU'`: percentage from BAU of the same year\n",
    "    * `'diff_BAU'`: absolute difference between scenario and year from BAU of the same year\n",
    "    * `'percent_change_ELM'`: percentage change from ELM of the same year\n",
    "    * `'diff_ELM'`: absolute difference between scenario and year from ELM of the same year\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "fp = '../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/GLOBIOM-full_250819.csv'\n",
    "el2_pipeline(fp)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# example output of el2_pipeline(fp)\n",
    "\n",
    "PROCESSING FILE : model\n",
    ">> original DataFrame length: 745958\n",
    "\n",
    "\n",
    ">> checking duplicates\n",
    "Found 70 duplicated entries\n",
    "...0 of them have conflicting values...\n",
    "All files will be saved in: ../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/duplicates\n",
    "\n",
    "\n",
    ">> setting aside variables to keep\n",
    "... set aside variables to keep. DataFrame length: 694111, 93.0% of the original df\n",
    ">> checking overrides\n",
    "... no overrides file found!\n",
    "\n",
    "\n",
    "\n",
    ">> checking against template\n",
    "Template exceptions removed: 0\n",
    "... template checked. DataFrame length: 669810, 90.0% of the original df\n",
    "... concatenating template-checked DataFrame with the kept overrides and variables to keep...\n",
    ">> checking duplicates again\n",
    "Found 0 duplicated entries\n",
    "...0 of them have conflicting values...\n",
    "... DataFrame length: 721587, 97.0% of the original df\n",
    "All files will be saved in: ../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/template-checked\n",
    "\n",
    "\n",
    ">> calculating percentage changes\n",
    "All files will be saved in: ../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff\n",
    "Processing file: model_template-checked\n",
    "All files will be saved in: ../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/logs\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On multiple submissions (multiprocessed) ([source](../applepy/pipeline/pipeline.py#L160))\n",
    "\n",
    "**NOTE:** save submission files in a folder. `el2_pipeline_multiprocess` takes in a path to a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "data_dir = '../data/[folder of submissions]'\n",
    "el2_pipeline_multiprocess(data_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Additional Emission and Land calculations\n",
    "\n",
    "### Additional Emissions Calculations ([source](../applepy/utils/calculations/emissions.py))\n",
    " Some models are able to report more types of non-CO2 emissions and include these in their reporting of total emissions. To standardize the greenhouse gasses reported in the total non-CO2 emissions, we calculate the total methane and nitrous oxide emissions reported by the models. The outputs of this additional calculations are the variables (note this is only done for `AGR`):\n",
    " * `EMIS_added`: added values for CO2, CH4, and N2O\n",
    " * `EMIS_nonCO2`: added values for CH4 and N2O\n",
    " * `ECH4_share`: share of CH4 in `EMIS_added`\n",
    " * `ECO2_share`: share of CO2 in `EMIS_added`\n",
    " * `EN2O_share`: share of N2O in `EMIS_added`\n",
    " * `nonCO2_share`: share of non-CO2 greenhouse gases (CH4 and N2O) in `EMIS_added`\n",
    "\n",
    "### Additional Land Calculations ([source](../applepy/utils/calculations/land.py))\n",
    " To standardize the reporting of agricultral land to include only cropland and grassland, we calculate additional variables that \n",
    " :\n",
    " * `LAND_added`: includes items:\n",
    "    * `AGR_added`: added values for cropland (`CRP`) and grassland (`GRS`)\n",
    "    * `ONV_added`: added values for other natural vegetation (`ONV`) and forests (`FOR`), if they were reported\n",
    "    * `LAND_tot`: `AGR_added` and `ONV_added`\n",
    "    * `CRP`: original `CRP` value reported\n",
    "    * `GRS`: original `GRS` reported\n",
    " * `LAND_share`: includes items:\n",
    "    * `CRP_share`: share of `CRP` in `LAND_tot`\n",
    "    * `GRS_share`: share of `GRS` in `LAND_tot`\n",
    "    * `AGR_share`: share of `AGR_added` in `LAND_tot`\n",
    "    * `ONV_share`: share of `ONV_added` in `LAND_tot`\n",
    "    * `CRP_AGR_share`: share of `CRP` in `AGR_added`\n",
    "    * `GRS_AGR_share`: share of `GRS` in `AGR_added`\n",
    "\n",
    "**NOTE:** all of these calulations are done in using the absolute values that each model reports. Following the summations, the percentage changes are calulated the same as the core dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "fp = \"../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/model_template-checked_pc-diff_interp-2020.csv\"\n",
    "run_emissions_calcs(fp)\n",
    "run_land_calcs(fp)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "## example output of additional emission and land calcs\n",
    "\n",
    "All files will be saved in: ../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/emissions\n",
    "\n",
    "Statistics on the difference between added emissions (CH4, N2O, and CO2) and total emisisons reported\n",
    "          count          mean           std       min           25%  50%  \\\n",
    "model                                                                      \n",
    "GLOBIOM  2052.0 -3.411326e-09  1.515930e-07 -0.000001 -5.684342e-14  0.0   \n",
    "\n",
    "                  75%       max  \n",
    "model                            \n",
    "GLOBIOM  5.684342e-14  0.000001  \n",
    "\n",
    ">> Saving wide DataFrame to ../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/emissions/model_template-checked_pc-diff_interp-2020_EMIS-calcs-w.csv\n",
    ">> Saving long DataFrame to ../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/emissions/model_template-checked_pc-diff_interp-2020_EMIS-calcs.csv\n",
    ">> Running percentage change calculations...\n",
    "Processing file: model_template-checked_pc-diff_interp-2020_EMIS-calcs\n",
    "All files will be saved in: ../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/emissions/logs\n",
    "100%|██████████| 108/108 [00:24<00:00,  4.33it/s]\n",
    "Done. Saving file to ../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/emissions/model_template-checked_pc-diff_interp-2020_EMIS-calcs_pc-diff_interp-2020.csv\n",
    "All files will be saved in: ../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/land\n",
    "\n",
    ">> Saving wide DataFrame to ../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/land/model_template-checked_pc-diff_interp-2020_LAND-calcs-w.csv\n",
    ">> Saving long DataFrame to ../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/land/model_template-checked_pc-diff_interp-2020_LAND-calcs.csv\n",
    ">> Running percentage change calculations...\n",
    "Processing file: model_template-checked_pc-diff_interp-2020_LAND-calcs\n",
    "All files will be saved in: ../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/land/logs\n",
    "100%|██████████| 162/162 [00:38<00:00,  4.22it/s]\n",
    "Done. Saving file to ../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/land/model_template-checked_pc-diff_interp-2020_LAND-calcs_pc-diff_interp-2020.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Merging and updating the dataset\n",
    "1. Merging\n",
    "2. Updating an                  existing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# using the function merge_fps([list of filepaths]) to merge datasets\n",
    "fps = [\"../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/model_template-checked_pc-diff_interp-2020.csv\", # core submission file\n",
    "       \"../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/emissions/model_template-checked_pc-diff_interp-2020_EMIS-calcs_pc-diff_interp-2020.csv\", # additional emissions calcs\n",
    "        \"../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff/land/model_template-checked_pc-diff_interp-2020_LAND-calcs_pc-diff_interp-2020.csv\" # additional land calcs\n",
    "       ] \n",
    "output_dir = '../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/pc-diff'\n",
    "check_path(output_dir)\n",
    "merge_fps(fps,save=True,output_dir=output_dir,merge_fn=f\"flw-update_GLOBIOM_emis-land-calcs_{time.strftime('%y%m%d')}.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "### Example for partial update: GLOBIOM's PARTIAL FLW UPDATE\n",
    "fp_old = '../data/2025-06-13_FLW-sensitivity/merged_files/flw-update_emis-land-calcs_250731.csv'\n",
    "df_old = pd.read_csv(fp_old,index_col=0)\n",
    "\n",
    "fp_new = '../data/2025-06-13_FLW-sensitivity/250819_GLOBIOM-partial-update/GLOBIOM_FOUR_WASTE_SCENARIOS_ONLY_08182025_104106.csv'\n",
    "df_new = AgMIP_read_raw_csv(fp_new)\n",
    "\n",
    "models_to_replace = df_new.model.unique()\n",
    "scenarios_to_replace = df_new.scenario.unique()\n",
    "\n",
    "# Merge GLOBIOM's new scenarios into the old one\n",
    "# so get all the entries for GLOBIOM, but not the ones for the scenarios that will be replaced\n",
    "df_old = df_old[(df_old.model.isin(models_to_replace)) &\n",
    "                (~df_old.scenario.isin(scenarios_to_replace))]\n",
    "\n",
    "cols = ['model', 'scenario', 'region', 'variable', 'item', 'unit', 'year',\n",
    "       'value']\n",
    "df_update = pd.concat([df_new, df_old]).reset_index(drop=True)[cols]\n",
    "        \n",
    "df_update.to_csv(\"/\".join(fp_new.split(\"/\")[:-1])+f\"/GLOBIOM-full_{time.strftime('%y%m%d')}.csv\",index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 (Optional): Model Specific adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with variables that report dry matter\n",
    "IMAGE and CAPRI report dry matter as separate variables. In this step, the dry matter variables (`*_dm`) units are replaced to add `dm` (e.g. `1000 t` to `1000 t dm`). Then the suffix is removed and combined to the base variable (e.g. `PROD_dry` to `PROD`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```python\n",
    "fp = \"../data/2025-06-13_FLW-sensitivity/pc-diff_all/merged/flw-update_250630.csv\"\n",
    "df = pd.read_csv(fp,index_col=0)\n",
    "\n",
    "#replace CAPRI *_dry to *_dm\n",
    "df['variable'] = df['variable'].str.replace(r'_dry$', '_dm', regex=True)\n",
    "\n",
    "# load dm_unit lookup table\n",
    "dm_units_fp = \"../applepy/template/dm_units.json\"\n",
    "with open(dm_units_fp) as json_data:\n",
    "    dm_units = json.load(json_data)\n",
    "    json_data.close()\n",
    "\n",
    "# get list of *_dm vars\n",
    "dm_vars = df[df.variable.str.endswith(\"_dm\")].variable.unique()\n",
    "\n",
    "# # check current data units: \n",
    "# for k in dm_units.keys():\n",
    "#     model_units = df[df.variable==k].unit.unique()\n",
    "#     print(k,model_units,dm_units[k])\n",
    "\n",
    "for v in dm_vars:\n",
    "    # replace units with dm_unit\n",
    "    df.loc[df.variable==v,'unit'] = dm_units[v]\n",
    "    \n",
    "    # remove '*_dm' suffix from variable\n",
    "    df.loc[df.variable==v,'variable'] = v.split('_dm')[0]\n",
    "    # print(v, model_units,df[df.variable==v].unit.unique(), dm_units[v])\n",
    "\n",
    "# save df\n",
    "df.to_csv(fp.split(\".csv\")[0]+'_dm-fixed.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Decomposition analysis ([source](applepy/utils/calculations/decomposition.py))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from applepy.utils.calculations.decomposition import *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# drop ELM_MITI and replace EL2 with ELM_MITI since these two scenarios are the same\n",
    "df = df[df.scenario!='ELM_MITI']\n",
    "# rename EL2 to ELM_MITI\n",
    "df.loc[df.scenario=='EL2','scenario'] = 'ELM_MITI'\n",
    "scenarios = ['BAU',\n",
    "             'BAU_PROD','BAU_WAST','BAU_DIET','BAU_MITI',\n",
    "             'ELM',\n",
    "             'ELM_PROD','ELM_WAST','ELM_DIET','ELM_MITI']\n",
    "\n",
    "df = df[df.scenario.isin(scenarios)]\n",
    "for model in df.model.unique():\n",
    "    print(model, sorted(df[df.model==model].scenario.unique()))\n",
    "\n",
    "variables = df.variable.unique()\n",
    "items = df.item.unique()\n",
    "regions = ['CAN','USA','BRA','OSA','FSU','EUR','MEN','SSA','CHN','IND','SEA','OAS','ANZ','WLD']\n",
    "years = [2050.0]\n",
    "df = df[(df.variable.isin(variables)) &\n",
    "        (df.item.isin(items))         &\n",
    "        (df.region.isin(regions))     &\n",
    "        (df.year.isin(years))         ]\n",
    "grouped = df.groupby(['model','region','variable','item','unit','year'])\n",
    "\n",
    "drivers = ['DIET','PROD','MITI','WAST']\n",
    "dc_dict = []\n",
    "values = ['value','percent_change_BAU','percent_change_BAU_ref_year'] \n",
    "for i, (model,region,variable,item,unit,year) in enumerate(tqdm(grouped.groups.keys())):\n",
    "    k_df = grouped.get_group((model,region,variable,item,unit,year))\n",
    "    for driver in drivers:\n",
    "        for value in values: \n",
    "            for normalized in [True,False]:\n",
    "                try:\n",
    "                    dc_dict.append(decompose_driver_effect_filtered(k_df,value,driver,normalized=normalized, use_pandas=True, full_dict=True))\n",
    "                except Exception as e:\n",
    "                    print(model,region,variable,item,unit,year,e)\n",
    "dc_df = pd.DataFrame.from_dict(dc_dict)\n",
    "cols = ['individual', 'total', 'interaction', 'ELM','EL2', 'BAU', 'ELM_driver', 'BAU_driver']\n",
    "for col in cols:\n",
    "    dc_df[col] = [np.nan if (x.size==0) or (np.isnan(x[0])) else x[0] for x in dc_df[col].values]\n",
    "dc_df.to_csv(pjoin(output_dir,base_filename+'_decomposed.csv'),index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# convert to long format\n",
    "dc_df_l = dc_df.melt(id_vars=['model','region','variable','item','unit','year','driver','normalized','value_type'],value_vars=['individual','total','interaction'],var_name='effect')\n",
    "# dc_df_l.value = [np.nan if (x.size==0) or (np.isnan(x[0])) else x[0] for x in dc_df_l.value.values]\n",
    "dc_df_l.dropna(inplace=True)\n",
    "dc_df_l.to_csv(pjoin(output_dir,base_filename+'_decomposed_l.csv'),index=False)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "el-modelling_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
